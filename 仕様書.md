SLMは本番はqwen3の8Bを使い
ノートPCで出先で実演する場合はqwen3の1.7B

エンベリングはcl-nagoya/ruri-v3-310mを使う


フィルターのモデル(分類モデル)はcl-tohoku/bert-base-japanese-whole-word-maskingにします(最新のものはどれがいいのかわからない)。



GeminiAPI関係

https://ai.google.dev/gemini-api/docs/document-processing?hl=ja&_gl=1*9sgfdc*_up*MQ..*_ga*MzU2ODMwNjI2LjE3NjAyNDk4OTk.*_ga_P1DBVKWT6V*czE3NjAyNDk4OTkkbzEkZzAkdDE3NjAyNDk5MDQkajU1JGwwJGg4NjU0Mzk3MjA.



https://ai.google.dev/gemini-api/docs/thinking?hl=ja&_gl=1*yc2rza*_up*MQ..*_ga*MTQ0NDEyMzc1OS4xNzYwMjUwNDA4*_ga_P1DBVKWT6V*czE3NjAyNTA0MDgkbzEkZzAkdDE3NjAyNTA0MDgkajYwJGwwJGg2NDQ3MzE2MDU.



https://ai.google.dev/gemini-api/docs/structured-output?hl=ja&_gl=1*diorkj*_up*MQ..*_ga*MTQ0NDEyMzc1OS4xNzYwMjUwNDA4*_ga_P1DBVKWT6V*czE3NjAyNTA0MDgkbzEkZzAkdDE3NjAyNTA0MjQkajQ0JGwwJGg2NDQ3MzE2MDU.



https://ai.google.dev/gemini-api/docs/text-generation?hl=ja&_gl=1*g94e9z*_up*MQ..*_ga*MTQ0NDEyMzc1OS4xNzYwMjUwNDA4*_ga_P1DBVKWT6V*czE3NjAyNTA0MDgkbzEkZzAkdDE3NjAyNTA0MjQkajQ0JGwwJGg2NDQ3MzE2MDU.



https://ai.google.dev/gemini-api/docs/batch-api?hl=ja&_gl=1*1gr129u*_up*MQ..*_ga*MTQ0NDEyMzc1OS4xNzYwMjUwNDA4*_ga_P1DBVKWT6V*czE3NjAyNTA0MDgkbzEkZzAkdDE3NjAyNTA1MzUkajYwJGwwJGg2NDQ3MzE2MDU.





論文取得にjstageAPIについてです。

https://www.jstage.jst.go.jp/static/pages/JstageServices/TAB3/-char/ja

https://www.jstage.jst.go.jp/static/files/ja/manual_api.pdf

https://note.com/ryu_y71/n/n74f8b67e4784

https://qiita.com/mamoru-ngy/items/a16a7abaf30ddef74cde

https://note.com/fair_eel843/n/nbc00837a286d

https://www.jstage.jst.go.jp/static/pages/WebAPI/-char/ja

https://qiita.com/trhstrhsr/items/61acea47c18d235534ed

https://ooooooha.hatenablog.com/entry/2017/05/03/023516



どこでもデータセットを作れるようにしたいと考えているのです。
学校のPCにデータを終結させる。
作成するデータセットの論文(RAG参照データ)一致はさせたいと思っています(サーバーが作ったmdを利用)。

VPNなどの利用はできません(学校以外の時間に、学校のPCは使えない)。


# **データセット自動生成アプリケーション仕様書 (1/5) - 基本構想とアーキテクチャ**


## 1. 概要 (Overview)

### 1.1 目的
本アプリケーションの主目的は、「リハビリテーション実施計画書 自動作成システム」のAIエンジンを、クラウドAPI（Gemini）から**完全にローカルで動作する複数の特化型SLM（小規模言語モデル）群へと移行**するために不可欠な、**高品質なファインチューニング用データセットを体系的かつ大規模に自動生成**することである。

### 1.2 スコープ
本アプリケーションは、以下の4種類のデータセットの生成をスコープとする。
1.  **RAG用ソースデータ**: 論文から抽出・整形した、RAGの知識源となる構造化Markdownファイル群。
2.  **項目生成LoRA用データセット**: 計画書の各項目を専門的に生成するSLMを学習させるための、対話形式（JSONL）の教師データ。
3.  **情報抽出SLM用データセット**: 自由記述テキストから計画書のチェックボックス等を自動入力するSLMを学習させるための、対話形式（JSONL）の教師データ。
4.  **Embeddingモデル用データセット**: RAGの検索精度を向上させるため、埋め込みモデルを専門分野に適応させるためのトリプレット形式データ。

*スコープ外*: 本アプリケーションはデータセットの生成までを担当し、生成されたデータセットを用いたモデルのファインチューニング処理そのものは含まない。

---
## 2. システムアーキテクチャ (System Architecture)

### 2.1 構成概要
システムの堅牢性、拡張性、および並列処理能力を最大化するため、**ハブ＆スポークモデル**を採用する。中央の司令塔（ハブ）がタスクを管理し、多数の作業員（ワーカー）が実際のデータ生成処理を分担する。各コンポーネントはHTTPプロトコルを介して通信する。司令塔（ハブPC）がAPIサーバーとなり、作業員（ワーカーPC）がクライアントとしてリクエストを送信する、シンプルなクライアント/サーバー構成とする。


### 2.2 各コンポーネントの役割
* **ハブPC (Hub PC / 司令塔)**
    * **役割**: システム全体の司令塔。タスクの生成、進捗管理、および成果物の一元管理を担当する。
    * **機能**:
        * J-STAGE APIを介した論文の検索・収集。
        * 収集した論文と患者ペルソナを組み合わせた「データ生成ジョブ」の作成。
        * 作成したジョブをAPIリクエストに応じてワーカーに提供。
        * ワーカーPCから返却された生成済みデータセットの集約、検証、および保存。
        * APIエンドポイントの状態監視とロギング。

* **ワーカーPC (Worker PC / 作業員)**
    * **役割**: 実際のデータ生成処理を実行する分散ユニット。
    * **機能**:
        * ハブPCのAPIにアクセスし、自律的にジョブを取得。
        * ジョブの指示に基づき、Gemini APIを呼び出し、論文の構造化や教師データの生成などの重い処理を実行。
        * 生成した成果物をハブPCに報告・返却。
        * 処理完了後、次のジョブをAPIに問い合わせる。



### 2.3 技術スタック
* **プログラミング言語**: Python 3.10以降
* **通信・APIサーバー**:
　* **Flask**: ハブPC側で、ワーカーPCからのリクエストを待ち受ける軽量なAPIサーバーを構築するために使用。
　* **Requests**: ワーカーPC側で、ハブPCのAPIサーバーにHTTPリクエストを送信するために使用。
* **データ生成**:
    * **Google Generative AI SDK (`google-genai`)**: Gemini APIを呼び出し、構造化出力機能などを活用するため。
    * **Pydantic**: Gemini APIに渡すJSONスキーマを定義し、出力結果を検証するため。
* **論文取得・解析**:
    * **J-STAGE API**: 論文のメタデータ検索と本文URLの取得。
    * **Requests**: Webページ（HTML）の取得。
    * **BeautifulSoup4**: HTMLコンテンツのパースとテキスト抽出。
    * **PyMuPDF**: PDFファイルからの高精度なテキスト抽出。
* **データ処理・保存**:
    * **Pandas**: 抽出したデータの整理や分析。
    * **JSONL (JSON Lines)**: ファインチューニング用データセットの標準的な保存形式。

---
## 3. データフロー

アプリケーション全体のデータの流れは以下の通りです。

1.  **[ステップ1：サーバー準備とジョブリスト作成（ハブPC）]**
    * `hub_server.py`を起動し、FlaskによるAPIサーバーを稼働させる。サーバーはワーカーからのHTTPリクエストを待ち受ける状態になる。
    * 並行して、J-STAGE APIから論文情報を収集し、処理すべきタスクのリスト（ジョブリスト）をメモリ上または簡易的なファイルに保持する。この時点では、各ジョブの状態は「未処理（pending）」となっている。

2.  **[ステップ2：ジョブ取得要求（ワーカーPC）]**
    * ワーカーPCで`worker_client.py`を起動する。
    * スクリプトは無限ループに入り、まずハブPCのAPIエンドポイント（例: `http://<ハブIP>:5000/get-job`）に対してHTTP GETリクエストを送信する。

3.  **[ステップ3：ジョブの割り当てとロック（ハブPC）]**
    * ハブPCは`/get-job`リクエストを受け取ると、ジョブリストの中から「未処理」のジョブを1つ選択する。
    * そのジョブの状態を「処理中（processing）」に更新し、どのワーカーが担当しているかを記録する（重複割り当て防止）。
    * 選択したジョブの内容（論文テキスト、ペルソナ等を含むJSON）を、HTTPレスポンスとしてワーカーPCに返却する。もし未処理のジョブがなければ、ステータスコード204（No Content）を返す。

4.  **[ステップ4：データ生成処理（ワーカーPC）]**
    * ワーカーPCは、ハブPCから受け取ったジョブのJSONデータを解析する。
    * 指示に従い、Gemini APIとの通信、テキストの整形、JSONの生成といった、**計算負荷の高い全ての処理**を自身のマシンリソースを使って実行する。

5.  **[ステップ5：成果物の報告（ワーカーPC）]**
    * 処理が正常に完了したら、ワーカーPCは生成した最終的なデータセット（JSONL形式の文字列など）と、完了ステータス（`completed`）を1つのJSONにまとめる。
    * この結果JSONを、ハブPCのAPIエンドポイント（例: `http://<ハブIP>:5000/submit-result`）にHTTP POSTリクエストで送信する。
    * もし処理中にエラーが発生した場合は、エラーステータス（`failed`）とエラーメッセージを送信する。

6.  **[ステップ6：結果の集約とループ（ハブPC & ワーカーPC）]**
    * ハブPCは`/submit-result`リクエストを受け取り、送信されたデータセットを適切なディレクトリに保存し、対応するジョブの状態を「完了（completed）」に更新する。
    * ワーカーPCは、結果を送信した後、短い待機時間を挟み、**ステップ2に戻って**再びハブPCに次の仕事を問い合わせる。このサイクルを、ハブPCから仕事がないという応答が返ってくるまで繰り返す。

---
# **データセット自動生成アプリケーション仕様書 - 補足資料：患者ペルソナの設計と生成**

## 4. 患者ペルソナの設計と生成

本アプリケーションで生成するデータセットの質と多様性を決定づける最も重要な要素が**「患者ペルソナ」**です。これは、単なるランダムなデータではなく、臨床現場の多様な実情を反映した、リアルな架空の患者像を指します。

### 4.1 目的と重要性

高品質な患者ペルソナを自動生成する目的は以下の通りです。

* **多様性の確保**: 1つの学術論文から、年齢・性別・社会的背景・希望などが異なる多様な臨床ケースをシミュレートし、データセットのバリエーションを飛躍的に増加させます。
* **汎化性能の向上**: モデルが特定の患者パターン（例：高齢男性の変形性膝関節症）に過学習（過剰適合）するのを防ぎます。多様なペルソナで学習させることで、モデルは未知の症例に対しても柔軟に対応できる**汎化性能**を獲得します。
* **臨床現場の再現性**: 理想的なテキストだけでなく、「転倒への恐怖心が強い」「家族の協力が得られにくい」といった臨床で頻繁に遭遇する心理的・社会的因子をペルソナに含めることで、より実践的な文章生成能力をモデルに学習させます。

### 4.2 ペルソナの構成要素

各ペルソナは、以下の要素で構成される構造化データ（JSON）として定義されます。

| 要素 | 内容 | 生成例 |
| :--- | :--- | :--- |
| **基本属性** | 年代と性別。ワーカーPCごとに事前に割り当てられる。 | `"年代": "70代"`, `"性別": "男性"` |
| **臨床情報** | 疾患名、重症度、病期、合併症など。 | `"疾患名": "変形性膝関節症"`, `"重症度": "K-L Grade 3"` |
| **背景・生活史** | 職業、家族構成、住環境、趣味、受傷・発症経緯など。 | `"背景": "長年農作業に従事。現在は妻と二人暮らし。趣味は盆栽。"` |
| **主観的情報** | 患者本人や家族の主な訴え、希望、目標。 | `"希望": "手術はせずに、痛みを和らげて再び畑仕事をしたい。"` |
| **心理・社会的因子** | 不安、意欲、キーパーソンなど、治療に影響を与える要素。 | `"心理的因子": "転倒への恐怖心が強く、外出をためらいがち。"` |

### 4.3 自動生成ワークフロー

患者ペルソナは、ハブPCがデータセット生成ジョブを作成する際に、以下のステップで自動生成されます。

1.  **基本属性の決定**:
    * ハブPCは、ジョブを割り当てるワーカーPCに事前設定された基本属性（例：`PC-01: 20代男性`, `PC-02: 20代女性`...）を読み込みます。

2.  **論文テーマの抽出**:
    * ジョブの対象となる論文テキストから、主要な疾患名やテーマ（例：「大腿骨頸部骨折」「ACL再建術後」）を抽出します。

3.  **LLMによる詳細プロファイルの生成**:
    * ハブPCは、上記1と2の情報を基に、Gemini APIに対して**「ペルソナ生成」**を指示します。この際、構造的出力機能とPydanticスキーマを活用し、必ず指定した形式でペルソナが出力されるようにします。

    **ペルソナ生成用のメタプロンプト例**:
    > あなたは経験豊富な臨床家です。以下の【条件】に合致する、リアルな架空の患者プロフィールを創作してください。出力は指定されたJSONスキーマに従ってください。
    >
    > **【条件】**:
    > * **基本属性**: 70代、男性
    > * **主要疾患**: 変形性膝関節症
    >
    > **生成するプロフィールの要件**:
    > * 臨床的にあり得るリアルな背景・生活史を含めること。
    > * 患者が抱える具体的な悩みや希望を記述すること。
    > * 治療の妨げ、または促進になりうる心理・社会的因子を1つ以上含めること。

4.  **ジョブへの組み込み**:
    * 生成された構造化ペルソナデータを、論文テキストと共に1つのジョブとしてパッケージ化し、API経由でワーカーPCに提供できるように準備します。

### 4.4 多様性確保のための戦略

単にランダムに生成するだけでなく、臨床的に意味のある多様性を確保するため、ペルソナ生成時には以下のようなバリエーションを持たせる戦略をとります。

* **病期・重症度の多様化**: 同じ疾患でも「術後急性期」「回復期」「生活期（慢性期）」や、「軽度」「中等度」「重度」といった異なるステージのペルソナを意図的に生成させます。
* **社会的背景の多様化**: 「独居」「老老介護」「子育て世代」「アスリート」「デスクワーカー」など、社会的役割や環境が異なるペルソナを生成します。
* **目標の多様化**: 「競技復帰」「職場復帰」といった高い目標から、「日常生活の自立」「痛みの緩和」といったQOL維持を目的とする目標まで、幅広いゴール設定を持つペルソナを作成します。

この精緻な患者ペルソナ生成プロセスを通じて、1つの論文からでも、まるで30人の異なる患者を担当するかのような、多角的で質の高いデータセットを大規模に生産することが可能になります。


---
# **データセット自動生成アプリケーション仕様書 - 補足事項**

## 5. 品質保証（QA）とヒューマン・イン・ザ・ループ

高品質なデータセットは、高性能なモデルを育成するための最も重要な資産です。AIによる自動生成は効率的ですが、その品質を保証するため、人間が介在する仕組み（Human-in-the-Loop）を設計に組み込みます。

### 5.1 データレビューシステムの導入
* **目的**: ワーカーPCが生成したデータセット（特に、`input`と`output`のペア）の品質を、人間が効率的にチェック・修正するための簡易的なWebインターフェースをハブPC上に構築します。
* **機能**:
    1.  **サンプリング表示**: ハブPCは、集約したデータセットからランダムに、または特定の基準（例：文章が極端に短い）でサンプルを抽出し、レビュー担当者に提示します。
    2.  **並列比較表示**: レビュー画面では、「元論文」「生成された臨床サマリー（入力）」「生成されたJSON（出力）」を並べて表示し、内容の整合性や妥当性を直感的に確認できるようにします。
    3.  **簡易修正と承認**: レビュアーは、軽微な誤り（誤字脱字、不自然な表現など）をその場で修正し、「承認」ボタンを押すことで、そのデータを確定済みのデータセットに移動させます。明らかに質の低いデータは「却下」できます。
* **重要性**: このレビュープロセスを通じて、AIの生成傾向（特定の疾患で間違いが多いなど）を把握し、メタプロンプトを改善するための貴重なフィードバックを得ることができます。

## 6. 堅牢なエラーハンドリングとロギング

30台のPCが連携する分散システムでは、個々のPCのエラーがシステム全体の停止につながらないように、詳細なエラーハンドリングとロギングの仕組みが不可欠です。

### 6.1 ジョブのステータス管理
* ハブPCのジョブ管理リスト内で、各ジョブは以下のステータスを持つように設計します。
    * `pending`: 未処理
    * `processing`: ワーカーが処理中
    * `completed`: 正常に完了
    * `failed`: 処理中にエラーが発生
* ワーカーPCは、処理を開始する際にジョブを`processing`に、完了・失敗時にそれぞれ`completed`・`failed`に更新します。

### 6.2 自動リトライとデッドレターキュー
* **自動リトライ**: ネットワークの一時的な問題やAPIの不安定さによる失敗に備え、ワーカーPCは失敗したジョブを**一定回数（例：3回）まで自動で再試行**します。
* **デッドレターキュー (DLQ)**: 何度再試行しても成功しないジョブ（例：論文URLがリンク切れ、論文の形式が特殊で解析不能など）は、**「失敗ジョブ専用のキュー（DLQ）」**に隔離します。これにより、問題のある1つのジョブがシステム全体の処理を滞らせることを防ぎます。開発者は後からDLQの中身を確認し、問題の原因を分析できます。

### 6.3 統合ログ管理
* 各ワーカーPCは、自身の処理ログ（どのジョブをいつ開始し、成功したか、エラー内容は何か）を**ハブPC上の共有ログサーバー**に送信します。
* ハブPCは、全PCからのログを一元的に集約・表示するダッシュボード機能を持つことで、システム全体の稼働状況やエラーの発生箇所をリアルタイムで把握できるようにします。

## 7. セキュリティと倫理的配慮

学術論文を扱う上で、著作権と倫理的な配慮は避けて通れません。

### 7.1 著作権の遵守
* **スクレイピング**: J-STAGE APIの利用規約や、各論文サイトの`robots.txt`を遵守し、サーバーに過度な負荷をかけないよう、リクエスト間隔を十分に設ける（例：1秒以上）ことをアプリケーションの仕様とします。
* **データセットの取り扱い**: 生成されたデータセットは、あくまで**組織内部での研究開発目的**にのみ利用し、外部に公開・配布しないことを原則とします。ファインチューニング済みのモデルを公開する場合も、学習データそのものが含まれないように注意します。

### 7.2 生成データの倫理的配慮
* **患者ペルソナ**: 生成される患者ペルソナは完全に架空のものであり、実在の個人情報を含まないように設計します。
* **バイアスの監視**: LLMは学習データに含まれるバイアスを再生産する可能性があります。生成されるデータセットに、特定の性別、年齢、社会的背景に対する偏見が含まれていないか、品質保証のプロセスで定期的に監視します。

---

# **データセット自動生成アプリケーション仕様書 (2/5) - パイプライン1：RAG用ソースデータ生成**

## 1\. 目的 (Objective)

本パイプラインの目的は、外部の学術論文リポジトリ（主にJ-STAGE）から運動器リハビリテーション分野の論文を取得し、それらをローカルSLMのRAG（Retrieval-Augmented Generation）機能で利用するのに最適な、**一貫性のある構造化Markdown形式のナレッジベースを構築**することである。

生成されるMarkdownは、単なるテキストではなく、見出し構造、図表、数式などが機械可読な形式で表現されており、後続のデータセット生成パイプラインの高品質な入力データとしても機能する。

## 2\. 入出力定義 (Input/Output)

  * **入力 (Input)**:

      * **検索キーワードリスト**: J-STAGE APIで論文を検索するためのキーワード群。（例: `["変形性膝関節症", "大腿骨頸部骨折", "運動療法", "物理療法"]`）
      * **処理対象論文URLリスト**: 手動で収集した、またはAPI検索で得られた論文の直接URLのリスト。

  * **出力 (Output)**:

      * **構造化Markdownファイル (`.md`)**: 論文ごとに1ファイルが生成される。ファイル名は、論文の識別子（DOIなど）を基に一意に命名される。
      * **処理結果ログ**: 各論文の処理成功、失敗、スキップなどの状態を記録したログファイル。

## 3\. 技術スタック (Technology Stack)

| 工程 | 主要技術・ライブラリ | 目的・役割 |
| :--- | :--- | :--- |
| **論文検索・取得** | `requests` | J-STAGE APIへのリクエスト送信、HTML/PDFファイルのダウンロード。 |
| **HTML解析** | `BeautifulSoup4` | HTML形式の論文から本文テキストや構造を抽出。 |
| **PDF解析** | `PyMuPDF` | PDF形式の論文から高精度にテキストや画像を抽出。 |
| **構造化・整形** | **Gemini API** | 抽出した生テキストを、厳格なルールに基づき高品質なMarkdownに変換。**構造化出力**機能はここでは使用せず、テキスト生成能力を主として利用する（[参考資料](https://ai.google.dev/gemini-api/docs/text-generation?hl=ja)）。 |
| **タスク管理** | `Flask`, `Requests` | ハブPCとワーカーPC間のHTTP通信によるタスクの割り当てと結果の集約。 |


## 4\. 詳細ワークフロー (Detailed Workflow)

本パイプラインは、ハブPCが提供するAPIに対し、ワーカーPCがリクエストを送信する形で実行される。

### ステップ1：論文検索とジョブリストの準備（ハブPC）

1.  **キーワードに基づく検索**:
      * ハブPC上のスクリプトが、定義されたキーワードリストを用いてJ-STAGE APIの検索エンドポイント（[参考資料](https://www.jstage.jst.go.jp/static/pages/JstageServices/TAB3/-char/ja)）を叩き、論文のメタデータ（タイトル, DOI, 本文URLなど）を取得する。
2.  **重複排除**:
      * 取得した論文リストをデータベースと照合し、既に処理済みの論文は除外する。
3.  **ジョブの生成**:
      * 未処理の各論文URLに対して、一意のジョブIDを付与した「Markdown生成ジョブ」を作成する。
      * ジョブの中身は、処理対象のURLと論文のメタデータを含むJSON形式とする。
      * 作成した全てのジョブを、APIサーバー内部のジョブ管理リスト（メモリ上またはファイル）で保持する。

### ステップ2：論文コンテンツの取得とテキスト抽出（ワーカーPC）

1.  **ジョブの取得**:
      * 各ワーカーPCは、ハブPCの **/get-job APIにリクエストを送信**し、処理すべきジョブ（URLとメタデータを含むJSON）を取得する。
2.  **コンテンツのダウンロード**:
      * ジョブに含まれるURLにアクセスし、論文のコンテンツ（HTMLまたはPDF）をダウンロードする。
      * **リクエスト間隔**: J-STAGEのサーバーに負荷をかけないよう、リクエストごとに1〜2秒の待機時間(`time.sleep`)を設けることを必須とする。
3.  **テキスト抽出**:
      * **HTMLの場合**: `BeautifulSoup4`を用いて、本文が含まれる主要なHTMLタグ（例: `<div class="main-text">`）を特定し、テキストを抽出する。ヘッダー、フッター、広告などのノイズは可能な限り除去する。
      * **PDFの場合**: `PyMuPDF`を用いて、ページごとにテキストを抽出する。2段組の論文にも対応し、正しい順序でテキストを結合する処理を行う。

### ステップ3：Gemini APIによる構造化Markdownの生成（ワーカーPC）

1.  **メタプロンプトの構築**:

      * 抽出した生テキストを基に、Gemini APIに投げるための詳細な指示プロンプト（メタプロンプト）を構築する。このプロンプトの品質が、出力されるMarkdownの品質を直接決定する。

    **メタプロンプト・テンプレート**:

    ```
    あなたは、学術論文を構造化されたMarkdownに変換するエキスパートです。以下の【論文テキスト】を解析し、【出力ルール】に厳密に従ってMarkdown形式で出力してください。論文の論理構造を維持することが最も重要です。

    【出力ルール】
    - 見出し：論文の章、節、項に対応する見出しを`#`, `##`, `###`で正確に階層化してください。
    - 図と表：図表は`![図1：〇〇の構成図]`のように、必ず内容を要約した説明的なキャプションを付けてください。
    - フローチャート：論文中の処理フローや意思決定ツリーは、Mermaid.jsの`graph TD`または`flowchart TD`記法で表現してください。
    - 数式：数式はインラインなら`$E=mc^2$`、ブロックなら`$$...$$`のようにLaTeX形式で記述してください。
    - 引用と参考文献：本文中の引用表記（例: [1]）はそのまま維持し、文末の参考文献リストは削除してください。
    - その他：上記以外の本文は、句読点や段落構造を維持したまま、プレーンテキストとして記述してください。

    【論文テキスト】
    [ここにステップ2で抽出・整形したテキストを挿入]
    ```

2.  **API呼び出しと結果の取得**:

      * 構築したプロンプトをGemini API（例: `gemini-2.5-flash`）に送信し、生成されたMarkdownテキストを取得する。

### ステップ4：成果物の保存と完了報告（ワーカーPC → ハブPC）

1.  **成果物の送信**:
      * 生成されたMarkdownテキストを、ジョブIDや完了ステータスと共にJSONに含め、ハブPCの **/submit-result APIにPOSTリクエストで送信する**。
2.  **完了処理**:
      * ワーカー側の処理はここまで。完了報告はPOSTリクエストに含まれる
      * もし、このプロセス中に回復不能なエラー（例：404 Not Found, テキスト抽出の完全な失敗）が発生した場合は、`failed`として報告し、エラー内容をログに記録する。

-----

# **データセット自動生成アプリケーション仕様書 - 補足事項 (2.1/5)**

## 5\. 品質の安定化と検証 (Quality Assurance)

本パイプラインの成功は、生成されるMarkdownの品質の一貫性にかかっています。そのため、以下の品質管理策を仕様に加えます。

### 5.1 コンテンツ抽出パターンの複数定義

  * **課題**: J-STAGE上の論文でも、ジャーナル（学会誌）によってHTMLの構造が微妙に異なります。単一の抽出ルール（例：`<div class="main-text">`を探す）だけでは、多くの論文でテキストの取りこぼしが発生する可能性があります。
  * **対策**:
    1.  **テンプレートの事前分析**: 主要なターゲットとなる数十のジャーナルを事前に分析し、本文が格納されているHTML構造の**パターンを複数特定**します（例：`パターンA: #main-content p`, `パターンB: .article-body`など）。
    2.  **優先順位付きセレクタ**: ワーカーPCは、論文をスクレイピングする際に、これらのパターンを優先順位順に試し、最初に本文テキストが取得できたルールを採用します。
    3.  **フォールバック**: どのパターンにも一致しなかった場合、`<body>`タグ以下の全テキストを抽出した後、Gemini APIに「この記事から本文だけを抽出して」と依頼するフォールバック処理を実行します。

### 5.2 メタデータの埋め込み（YAML Frontmatter）

  * **課題**: Markdownファイルには、後工程のRAGやデータセット生成で利用するための重要なメタデータ（論文の出典情報など）が必要です。

  * **対策**: 生成される各Markdownファイルの先頭に、**YAML Frontmatter**形式でメタデータを埋め込むことを必須仕様とします。これにより、ファイル自体が自己記述的なデータベースとなります。

    **Markdownファイルの出力例**:

    ```markdown
    ---
    title: "変形性膝関節症に対する運動療法の効果"
    doi: "10.1234/j.phys.ther.2025.xx"
    journal: "理学療法学"
    published_date: "2025-04-01"
    source_url: "https://www.jstage.jst.go.jp/..."
    ---

    # 緒言
    変形性膝関節症（KOA）は、加齢に伴う代表的な運動器疾患の一つである。...

     運動療法の種類
    運動療法には、筋力増強訓練、有酸素運動、関節可動域訓練などが含まれる。...
    ```

## 6\. 運用効率とコスト管理 (Operational Efficiency & Cost Management)

本パイプラインは大量のAPIコールを発生させるため、コストと実行時間を管理する仕組みが重要です。

### 6.1 バッチ処理APIの活用

  * **課題**: 論文1本ごとにGemini APIを呼び出すと、大量のリクエストが発生し、APIのレート制限（1分あたりのリクエスト数）に抵触しやすくなります。
  * **対策**: ご提示の[Batch APIの資料](https://ai.google.dev/gemini-api/docs/batch-api?hl=ja)を活用します。ワーカーPCは論文テキストを抽出した後、すぐにAPIを呼び出すのではなく、**一定数（例：20件）のプロンプトをバッチとしてまとめ**、一度のリクエストでAPIに送信します。これにより、API呼び出しのオーバーヘッドを削減し、レート制限への到達を緩和します。

### 6.2 キャッシュ機構の導入

  * **課題**: デバッグや再実行の際に、既に処理済みの論文に対しても再度APIコールが発生し、無駄なコストと時間がかかります。
  * **対策**: ハブPC上にシンプルなキャッシュデータベース（例：SQLiteやRedis）を構築します。
      * **キー**: 論文のURLまたはDOI
      * **値**: 生成されたMarkdownのコンテンツ
      * ワーカーPCはジョブを開始する前に、まずハブPCのキャッシュに問い合わせ、ヒットすればAPIコールをスキップしてキャッシュされた結果を利用します。

## 7\. 再現性とバージョン管理 (Reproducibility & Versioning)

  * **課題**: メタプロンプトや使用するGeminiモデルのバージョンを変更した場合、どのMarkdownがどの設定で生成されたものか追跡できなくなります。
  * **対策**:
      * **設定のスナップショット**: 生成されるMarkdownのYAML Frontmatterに、生成時に使用した**メタプロンプトのバージョン**や**モデル名**（例: `gemini-1.5-flash`）を記録します。
      * **出力ディレクトリのバージョニング**: `output/rag_source_v1/`, `output/rag_source_v2/` のように、大きな変更があった場合は出力先ディレクトリを分けることで、過去のデータセットを破壊することなく、新しいバージョンを生成できるようにします。

-----

# **データセット自動生成アプリケーション仕様書 (3/5) - パイプライン2：項目生成LoRA用データセット生成**

## 1\. 目的 (Objective)

本パイプラインの目的は、リハビリテーション実施計画書の**各項目（約20種類）を専門的に生成する、個別のLoRAアダプタ**をファインチューニングするための、高品質な教師データセット（JSONL形式）を大規模に自動生成することである。

生成されるデータは、単なるテキストのペアではなく、**計画書作成の論理的な順序と文脈**をモデルに学習させるため、「患者情報」「関連論文」「先行する項目の生成結果」を入力とし、次の項目を出力とする構造を持つ。

## 2\. 入出力定義 (Input/Output)

  * **入力 (Input)**:

      * **RAG用ソースデータ**: パイプライン1で生成された、構造化Markdownファイル群。
      * **患者ペルソナ生成定義ファイル**: 多様な患者像を生成するための基本設定ファイル（例：疾患ごとの年代・性別の分布、背景のキーワードなど）。
      * **項目生成順序定義ファイル**: 計画書項目をどの順序で生成するかを定義したリスト（例：`['main_risks_txt', 'goals_1_month_txt', ...]`）。

  * **出力 (Output)**:

      * **項目別JSONLファイル**: ファインチューニング対象の項目ごとに分割された、Alpaca形式の教師データファイル群（例：`main_risks_txt.jsonl`, `goals_1_month_txt.jsonl`）。
      * **処理結果ログ**: 各ジョブの成功・失敗、使用したプロンプト、APIからのレスポンスなどを記録した詳細ログ。

## 3\. 技術スタック (Technology Stack)

| 工程 | 主要技術・ライブラリ | 目的・役割 |
| :--- | :--- | :--- |
| **タスク管理** | `Flask`, `Requests` | ハブPCとワーカーPC間のHTTP通信によるタスクの割り当てと結果の集約。 |
| **データ生成** | **Gemini API** | **構造的出力機能**をフル活用し、Pydanticスキーマに基づいた正確なJSON（教師データ）を生成。 |
| **スキーマ定義** | `Pydantic` | `schemas.py`内のモデル（例：`RisksAndPrecautions`）を、Gemini APIへの出力形式指定として再利用。 |
| **RAG（文脈検索）** | `ChromaDB` (or `FAISS`) | 論文の中から、患者ペルソナに最も関連性の高い部分を検索し、プロンプトの`input`に含めるための簡易的なRAG機能。 |

## 4\. 詳細ワークフロー (Detailed Workflow)

このパイプラインは、\*\*「1論文 × 1患者ペルソナ ＝ 1計画書分の教師データセット」\*\*を1つの単位として生成するジョブを基本とする。

### ステップ1：コンテキストリッチなジョブの生成（ハブPC）

1.  **論文の選択と関連箇所の抽出**:
      * RAG用ソースデータから論文Markdownを1つ選択する。
      * 論文のタイトルや要旨から、主要な疾患名やテーマを抽出する。
2.  **患者ペルソナの生成**:
      * 論文のテーマ（例：「変形性膝関節症」）と、事前定義された属性（例：「70代女性」）に基づき、補足資料で定義したワークフローに従って、詳細な患者ペルソナ（JSON）をGemini APIで生成する。
3.  **RAGによる文脈情報の検索**:
      * 生成したペルソナのキーワード（疾患名、背景など）をクエリとして、対象論文のMarkdownファイル内を簡易的に**RAG検索**する。これにより、論文全体ではなく、**その患者に最も関連性の高い数段落**を抽出する。
4.  **連鎖的ジョブの作成と投入**:
      * ハブPCは、**項目生成順序**に従って、**一連のタスクを生成**する。最初のジョブは「リスク管理」項目用、2番目のジョブは「短期目標」項目用、といった具合である。
      * **重要な点**: 2番目以降のジョブには、「先行する項目の模範解答が必要である」という依存関係情報を含める。
      * これらのジョブをAPIサーバー内のジョブ管理リストで保持する。

**ジョブ（メッセージ）の例 (`goals_1_month_txt`生成用)**:

```json
{
  "job_id": "paper123_persona_01_item_02",
  "persona": { "年齢": "70代女性", "疾患": "変形性膝関節症", ... },
  "rag_context": "[論文から抽出した関連性の高いテキスト]...",
  "target_item": "goals_1_month_txt",
  "depends_on_items": ["main_risks_txt", "main_contraindications_txt", ...], 
  "output_schema": "Goals" // schemas.pyのPydanticクラス名
}
```

### ステップ2：文脈を考慮した教師データの逐次生成（ワーカーPC）

1.  **ジョブの取得と依存関係の解決**:

      * ワーカーPCはジョブを取得する。もし`depends_on_items`が存在する場合、ハブPCの結果保存領域を参照し、先行する項目の生成結果（模範解答）が揃うまで待機するか、（依存する項目が未完了の場合）ハブPCは新しいジョブを渡さず、ワーカーは少し待ってから再度リクエストする。

2.  **メタプロンプトの構築**:

      * 必要な情報がすべて揃ったら、Gemini APIに投げるためのプロンプトを構築する。このプロンプトには、\*\*入力（`input`）と指示（`instruction`）\*\*の両方が含まれる。

    **メタプロンプト例 (`goals_1_month_txt`生成時)**:

    > あなたはLoRAファインチューニング用の高品質な教師データを作成する専門家です。以下の【入力データ】を基に、【指示】に従って【出力データ】を生成してください。出力は指定されたJSONスキーマに厳密に従ってください。

    > **【入力データ】**:

    > ```json
    > {
    >   "patient_data": { "年齢": "70代女性", ... },
    >   "rag_context": "運動療法は疼痛軽減に有効であるとの報告がある...",
    >   "previously_generated": {
    >     "main_risks_txt": "高齢のため転倒リスクが高い。訓練は手すり内で開始する。",
    >     ... // 先行する全項目の生成結果
    >   }
    > }
    > ```

    > **【指示】**:
    > 上記の入力データを基に、リハビリテーション計画書の「短期目標 (`goals_1_month_txt`)」と「終了時目標 (`goals_at_discharge_txt`)」の項目を生成せよ。

    > **【出力スキーマ】**: `Goals` (Pydanticモデル)

3.  **構造化出力による生成**:

      * ワーカーPCは、このプロンプトと`response_schema`（`schemas.py`の`Goals`クラス）を指定してGemini APIを呼び出す。
      * APIから返された完璧なJSONが、教師データの\*\*`output`\*\*部分となる。

4.  **成果物の整形と報告**:

      * `instruction`, `input`, `output`を組み合わせた最終的なJSONL形式のデータを生成し、ハブPCの集約場所に保存する。
      * ハブPCの /submit-result APIに結果をPOST送信する。

このパイプラインを通じて、各項目が先行する項目を文脈として参照する、**論理的に一貫した**教師データセットを大規模に構築することが可能になります。

-----

# **データセット自動生成アプリケーション仕様書 - 補足事項 (3.1/5)**

## 5. 品質保証（QA）とフィードバックループ (Quality Assurance & Feedback Loop)

本パイプラインで生成される教師データの品質は、最終的なLoRAモデルの性能に直結します。そのため、AIによる自動生成プロセスに、品質を自己検証・向上させるための仕組みを組み込みます。

### 5.1 生成データの自己修正（セルフコレクション）メカニズム
* **課題**: Gemini APIも完璧ではなく、時には指示に反した出力（例：論理的に矛盾する内容、不自然な言い回し）を生成する可能性があります。
* **対策**: ワーカーPCは、教師データを1件生成するたびに、**自己評価のための追加APIコール**を実行します。これはご提示いただいた[思考の連鎖に関する資料](https://ai.google.dev/gemini-api/docs/thinking?hl=ja)の応用です。

    **自己評価プロンプトの例**:
    > あなたは品質評価の専門家です。以下の【入力データ】とそれに基づいて生成された【出力データ】を比較し、以下の基準で評価してください。
    > 1.  **一貫性**: 出力は、入力（特に`previously_generated`の内容）と論理的に矛盾していませんか？
    > 2.  **妥当性**: 出力は、臨床的に妥当な内容ですか？
    > 3.  **指示への準拠**: 出力は、`instruction`で与えられた指示に完全に従っていますか？
    >
    > もし問題があれば、**問題点を指摘し、修正した完璧なJSON**を出力してください。問題がなければ、元のJSONをそのまま出力してください。
    >
    > **【入力データ】**: `...`
    > **【出力データ】**: `...`

* **効果**: この「生成→自己評価→修正」のループを挟むことで、明らかなエラーや矛盾を含む低品質なデータが最終的なデータセットに混入するのを自動的に防ぎます。

### 5.2 計画書単位での一貫性最終検証
* **課題**: 項目ごとに生成を進めるため、初期の項目と最終的な項目との間で微妙なニュアンスのズレや矛盾が生じる可能性があります。
* **対策**: ハブPCは、1つの患者ペルソナに対する全ての項目（約20件）のデータ生成が完了した時点で、**全体の一貫性を検証する最終チェック**を行います。
* **方法**:
    1.  生成されたアウトプット群から、特に論理的な関連性が強い項目（例：「`main_risks_txt`」と「`policy_treatment_txt`」）をいくつかサンプリングします。
    2.  Gemini APIに対し、「『リスクは転倒リスクが高い』とあるのに、『治療方針は筋力強化のみ』となっています。これらは一貫していますか？」といった形で、矛盾がないかを確認させます。
    3.  矛盾が検出された場合は、そのデータセット全体に「レビュー要」フラグを立て、人間の目による最終確認を促します。

## 6. プロンプトとコンテキスト管理の高度化

### 6.1 RAGコンテキストの動的最適化
* **課題**: パイプライン(3/5)の仕様では、`rag_context`として論文の関連箇所を渡しますが、長すぎるとコスト増やノイズの原因になります。
* **対策**: ワーカーPCは、論文から抽出した関連箇所（例：5段落）をそのままプロンプトに含めるのではなく、**もう一段階処理**を挟みます。
    1.  まず、Gemini APIに「この5つの段落の中で、この患者ペルソナに最も重要な知見を**2つ**だけ選び、要約してください」と依頼します。
    2.  この要約された、より凝縮されたテキストを最終的な`rag_context`としてプロンプトに含めます。
    * これにより、コンテキストウィンドウを節約しつつ、モデルが最も重要な情報に集中できるようになります。

### 6.2 役割（Role）プロンプティングの活用
* **課題**: 生成される文章の品質や専門性のレベルが、LLMの解釈によってばらつく可能性があります。
* **対策**: 各項目の`instruction`に、より具体的な**役割（ペルソナ）**を明確に記述します。
    * **例（`main_risks_txt`用）**: `「あなたは、新人セラピストに指導を行う、経験20年のリスク管理担当の理学療法士です。...」`
    * **例（`goals_at_discharge_txt`用）**: `「あなたは、患者本人とその家族に対し、退院後の生活について分かりやすく説明するソーシャルワーカーです。...」`
    * このように役割を細かく設定することで、各項目に求められる文体や視点をより安定して生成させることができます。

---
# **パイプライン3：情報抽出SLM用データセット生成**

## 1. 目的 (Objective)

本パイプラインの目的は、`patient_info_parser.py`の機能をローカルSLMで代替するために、**臨床現場で書かれるような非構造化テキスト（カルテサマリー等）**と、それに完全に対応する**構造化JSONデータ（正解ラベル）**のペアからなる、高品質なファインチューニング用データセットを自動生成することである。

## 2. 入出力定義 (Input/Output)

* **入力 (Input)**:
    * **RAG用ソースデータ**: パイプライン1で生成された、構造化Markdownファイル群。
    * **患者ペルソナ生成定義ファイル**: 多様な患者像を生成するための基本設定。

* **出力 (Output)**:
    * **情報抽出用JSONLファイル (`parser_finetune_dataset.jsonl`)**: 対話形式（`messages`）で構成された、ファインチューニング用の教師データ。

## 3. 技術スタック (Technology Stack)

| 工程 | 主要技術・ライブラリ | 目的・役割 |
| :--- | :--- | :--- |
| **データ生成** | **Gemini API** | **テキスト生成**能力を用いて架空の臨床サマリーを作成し、**構造的出力**機能で完璧なJSON正解ラベルを生成する。 |
| **スキーマ定義** | `Pydantic` | `schemas.py`の`PatientMasterSchema`（または分割スキーマ群）を、正解ラベルの形式保証のために使用する。 |
| **タスク管理** | `Flask`, `Requests` | ハブPCとワーカーPC間のHTTP通信によるタスクの割り当てと結果の集約。 |

## 4. 詳細ワークフロー (Detailed Workflow)

本パイプラインは、1つの論文断片と1つの患者ペルソナから、1つの教師データペアを生成する**「2段階LLMコール方式」**を中核とする。

### ステップ1：ジョブの生成と投入（ハブPC）
1.  ハブPCは、RAG用ソースデータ（Markdown）からランダムにテキストブロック（数段落程度）を抽出する。
2.  抽出したテキストのテーマに合致する患者ペルソナを自動生成する。
3.  **「この論文断片とこのペルソナで、情報抽出用の教師データを1件作成せよ」**というジョブを生成し、APIサーバー内のジョブ管理リストで保持する。

### ステップ2：2段階LLMコールによるデータペア生成（ワーカーPC）

ワーカーPCはジョブを取得後、以下の2つのAPIコールを連続して実行する。

**【第1段階：架空の臨床サマリー（入力データ）の生成】**

1.  **目的**: 学術的な論文の記述を、臨床現場で使われる自然な要約文に変換する。
2.  **プロンプト**: Gemini APIに対し、論文断片と患者ペルソナを提示し、「この情報を基に、この患者の架空のカルテサマリーを臨床家の文体で作成してください」と依頼する（テキスト生成）。
3.  **得られる成果物**: `synthetic_clinical_summary.txt`（非構造化テキスト）。これがファインチューニング時の**入力データ (`user`側)** となる。

**【第2段階：構造化JSON（正解ラベル）の生成】**

1.  **目的**: 第1段階で生成したリアルなサマリーテキストを、完璧な構造化データに変換する。
2.  **プロンプト**: Gemini APIに対し、生成されたサマリーテキストを提示し、「このテキストから`PatientMasterSchema`に従って情報を抽出し、JSONで出力してください」と依頼する。この際、**構造的出力機能 (`response_schema`)** を使用する。
3.  **得られる成果物**: `ground_truth.json`。これがファインチューニング時の**正解データ (`assistant`側)** となる。

### ステップ3：最終的なデータセットの整形と保存
1.  ワーカーPCは、生成した「臨床サマリー」と「構造化JSON」のペアを、指定された対話形式（`messages`配列を持つJSONオブジェクト）に整形する。
2.  完成したデータを、ハブPCの`/submit-result`APIにPOST送信する。

---


# **パイプライン4：ファインチューニング用・統合データセット生成**

## 1\. 目的 (Objective)

RAGパイプラインの性能を決定づける3つの主要コンポーネント、すなわち**①Embeddingモデル（Bi-Encoder）**、**②リランカー（Cross-Encoder）**、そして\*\*③フィルター（軽量分類モデル）\*\*を、リハビリテーション分野の専門用語や文脈に特化させる（ドメイン適応させる）ための、高品質なファインチューニング用データセットを体系的かつ効率的に生成する。

本パイプラインは、**「One Source, Multi-Use（1つのソースを、多目的に利用する）」の原則に基づき、単一のデータ生成プロセスから得られる統合トリプレットデータセット**を、各モデルのファインチューニングスクリプトへの共通の入力として提供する。

## 2\. 入出力定義 (Input/Output)

  * **入力 (Input)**:

      * **RAG用ソースデータ**: パイプライン1で生成された、構造化Markdownファイル群。

  * **出力 (Output)**:

      * **統合トリプレットデータセット (`triplets.jsonl`)**: 各行が`query`, `positive`, `negative`のキーを持つJSONオブジェクトである、全てのファインチューニングの元となる唯一のデータセット。

## 3\. 技術スタック (Technology Stack)

| 工程 | 主要技術・ライブラリ | 目的・役割 |
| :--- | :--- | :--- |
| **正解ペア抽出** | カスタムPythonスクリプト | Markdownファイルの構造（見出しと本文）を解析し、`[query, positive]`ペアを網羅的に抽出する。 |
| **不正解文の採掘** | `rank_bm25` | キーワード検索エンジン（BM25）を利用し、各`query`に対して「キーワードは似ているが文脈は異なる」という、モデルの学習に最も効果的な**Hard Negative**（難しい不正解文）を効率的に探し出す。 |

## 4\. 詳細ワークフロー：統合トリプレットデータセットの生成

このプロセスは、主にハブPC上で実行されるバッチ処理として設計される。

**ステップ1：正解ペア `[query, positive]` の全件抽出**

1.  ハブPC上で実行されるスクリプトが、パイプライン1で生成された全てのRAG用Markdownファイル（`/output/rag_source_vX/`）をスキャンする。
2.  各ファイルから、\*\*見出し（`#`, `##`, `###`）**のテキストを**`query`\*\*として抽出する。
3.  その見出しに続く**最初の段落**を\*\*`positive`\*\*（正解文）として抽出し、`[query, positive]`のペアからなるリストをメモリ上に作成する。

**ステップ2：Hard Negative Miningのためのインデックス構築**

1.  ステップ1で抽出した全ての`positive`（本文段落のリスト）をコーパス（検索対象となるテキスト全体）とする。
2.  このコーパスを`rank_bm25`ライブラリに読み込ませ、高速なキーワード検索を可能にするためのインデックスをメモリ上に構築する。

**ステップ3：トリプレット `[query, positive, negative]` の生成と保存**

1.  ステップ1で作成した`[query, positive]`ペアのリストを一つずつループ処理する。
2.  ループ内の各`query`に対し、ステップ2で構築したBM25インデックスを検索する。
3.  BM25スコアの上位（例：トップ10）の検索結果の中から、**本来の`positive`文そのものを除いた**文章を1つランダムに選択し、これを\*\*`negative`\*\*（Hard Negative）とする。
4.  `query`, `positive`, `negative`の三つ組を1つのJSONオブジェクトとして、`triplets.jsonl`ファイルに追記保存していく。

**`triplets.jsonl` の出力イメージ**:

```json
{"query": "FAI術後の理学療法", "positive": "術後早期は可動域制限と疼痛管理に焦点を当てる...", "negative": "変形性股関節症では、保存療法としてまず運動療法が推奨される..."}
{"query": "脳卒中後の歩行能力の回復予測", "positive": "年齢、運動麻痺の重症度、下肢筋力が予測因子として報告されている...", "negative": "肩関節周囲炎の予後は、発症から寛解までに12か月から42か月を要する..."}
...
```

-----

## **（補足）5. 統合データセットの各モデルへの応用方法**

生成された**単一の `triplets.jsonl`** を、各モデルのファインチューニングスクリプトが、それぞれの学習タスクに合わせて**動的に解釈・利用**する。これにより、データセットの変換や複数管理が不要になる。

### **A) Embeddingモデル（Bi-Encoder）の学習**

  * **目的**: `query`と`positive`のベクトルを近づけ、`query`と`negative`のベクトルを遠ざける。
  * **データの解釈**:
      * 学習スクリプトは`triplets.jsonl`をそのまま1行ずつ読み込む。
      * `TripletLoss`のような損失関数が、3つの要素（アンカー=`query`, ポジティブ=`positive`, ネガティブ=`negative`）を直接受け取り、ベクトル空間上での距離を最適化するようにモデルを学習させる。

### **B) リランカー（Cross-Encoder）の学習**

  * **目的**: `[query, text]`のペアに対し、関連性が高い場合は`1.0`、低い場合は`0.0`に近いスコアを出力させる。
  * **データの解釈**:
      * 学習スクリプトは`triplets.jsonl`の1行から、**2つの学習サンプルをその場で生成**する。
        1.  `[query, positive]` のペア → ラベル `1.0` を付与
        2.  `[query, negative]` のペア → ラベル `0.0` を付与
      * これらの「入力ペア」と「正解ラベル」を使って、モデルを学習させる。

承知いたしました。2種類のフィルターの学習方法について、役割の違いが明確になり、誤解が生じないように説明を修正します。

---

### **C) フィルターモデル（軽量分類モデル）の学習**

**目的**: RAGパイプラインの検索結果から、無関係な情報を高速かつ正確に除去するための、**リハビリテーション分野に特化した2種類の軽量フィルターモデル**を構築します。

**データソース**: 生成した単一の`triplets.jsonl`を共通の入力として使用します。

---

#### **フィルター①：関連性分類モデル（Self-ReflectiveFilterの代替）**

* **モデルの役割**:
    `SelfReflectiveFilter`が担っていた「この文章は、質問に**関連があるか (Relevant) / ないか (Irrelevant)**」というシンプルな二値分類を、より高速かつ低コストで実行するための**専用モデル**です。Geminiのような大規模モデルに頼らず、BERTなどの軽量モデルでこの役割を代替します。

* **学習データの解釈**:
    学習スクリプトは`triplets.jsonl`の1行から、以下の2つの学習サンプルを動的に生成します。

    * `[クエリ, 正解文]`のペアから：
        * **入力**: `query`と`positive`のテキストペア
        * **正解ラベル**: `RELEVANT` (関連あり)

    * `[クエリ, 不正解文]`のペアから：
        * **入力**: `query`と`negative`のテキストペア
        * **正解ラベル**: `IRRELEVANT` (関連なし)

* **学習のゴール**:
    モデルに`query`と`passage`のペアを与えたとき、その関連性の有無を正しく分類できるように学習させます。

---

#### **フィルター②：NLIモデルのドメイン適応（NLIFilterの性能向上）**

* **モデルの役割**:
    既存の`NLIFilter`が使っている汎用的な自然言語推論（NLI）モデルを、**リハビリテーション分野の文脈に合わせて再学習（ドメイン適応）**させることで、判断精度を向上させます。このモデルは、「文章A（前提）は、文章B（仮説）を**論理的に支持するか (`entailment`)**、**矛盾するか (`contradiction`)**、**無関係か (`neutral`)**」を判断します。

* **学習データの解-釈**:
    学習スクリプトは`triplets.jsonl`の1行から、NLIタスク特有の形式に合わせて以下の学習サンプルを動的に生成します。

    * `[クエリ, 正解文]`のペアから：
        * **前提 (premise)**: `positive`（正解文）
        * **仮説 (hypothesis)**: `query`（クエリ）
        * **正解ラベル**: `entailment` (含意)
            * *(解釈: 「この正解文は、クエリという仮説を論理的に支持する内容である」)*

    * `[クエリ, 不正解文]`のペアから：
        * **前提 (premise)**: `negative`（不正解文）
        * **仮説 (hypothesis)**: `query`（クエリ）
        * **正解ラベル**: `neutral` (無関係)
            * *(解釈: 「この不正解文は、キーワードは似ているかもしれないが、クエリという仮説とは論理的に直接の関係はない」)*

* **学習のゴール**:
    一般的な文章だけでなく、専門用語が含まれるリハビリテーション分野の文章ペアに対しても、その論理的な関係性をより正確に推論できるようにモデルを特化させます。


---
# **データセット自動生成アプリケーション仕様書 (5/5) - 5. 運用・保守、および今後の拡張性**

## 5. 運用・保守 (Operations & Maintenance)

本アプリケーションは、数十台のPCが連携して長時間稼働する分散システムである。そのため、安定した運用と迅速な問題解決を可能にするための監視・保守体制を構築する。

### 5.1 統合ロギング戦略
* **目的**: 全てのPC（ハブおよびワーカー）で発生するイベントとエラーを一元的に収集・管理し、システムの稼働状況の可視化と、問題発生時の原因追跡を容易にする。
* **技術スタック**:
    * **ロギングライブラリ**: Python標準の`logging`モジュール。
    * **ログ集約**: ハブPC上に**Fluentd**や**Vector**といったログコレクターをセットアップし、各ワーカーPCからのログをTCP経由で受信する。
    * **ログストレージ/検索**: 集約したログは、**Elasticsearch**や**Loki**のような検索可能なデータベースに保存し、**Kibana**や**Grafana**といったダッシュボードツールで可視化する。
* **ログの内容**:
    * **ワーカーPC**: ジョブの開始/完了/失敗、処理時間、使用したLLMのトークン数、発生したエラーの詳細なトレースバック。
    * **ハブPC**: ジョブの生成とタスクキューへの投入状況、論文APIからのレスポンス、ファイルI/Oのエラー。
    * **ハブPC (Flaskサーバー)**: `/get-job`や`/submit-result`へのアクセスログ、ジョブの生成と割り当て状況、完了、リトライの状況、論文APIからのレスポンス、ファイルI/Oのエラー。

### 5.2 システム監視とアラート
* **目的**: システムの健全性をリアルタイムで監視し、異常が発生した際に管理者に即座に通知する。
* **監視ダッシュボード (Grafana/Kibana)**:
    * **全体進捗**: 全タスク数、完了数、失敗数、サーバーが管理する未処理ジョブ数を表示。
    * **ワーカー稼働状況**: 各ワーカーPCのCPU/GPU使用率、メモリ使用率、オンライン/オフライン状態を一覧表示。
    * **エラー発生率**: 時間あたりのエラー発生数をグラフ化し、急増を検知。
    * **API利用状況**: Gemini APIのコール数やエラーレートを監視。
* **アラート通知**:
    * **クリティカルエラー**: 特定のエラー（APIキー認証失敗、データベース接続不可など）が発生した場合、または失敗したジョブがデッドレターキュー（DLQ）に一定数溜まった場合に、管理者へメールやチャットツール（Slackなど）で即時通知する。

### 5.3 設定と成果物の管理
* **設定管理**: 各パイプラインのプロンプトテンプレート、患者ペルソナの定義、検索キーワードなどの設定ファイルは、**Git**でバージョン管理する。
* **成果物管理**: 生成されたデータセット（Markdown, JSONL）は、ハブPC上のストレージに、**生成日時と設定バージョン**がわかるようなディレクトリ構造で保存し、データの再現性を担保する (`/data/YYYYMMDD_v1.2/` のように）。

---
## 6. 将来の拡張性 (Future Extensibility)

本アプリケーションは、データセット生成に留まらず、将来的にはモデルの育成から評価までを自動化する**MLOps（Machine Learning Operations）プラットフォーム**へと進化するポテンシャルを持つ。

### 6.1 ファインチューニングプロセスの自動化
* **展望**: 現在は手動で実行しているファインチューニングのプロセスを、データセット生成パイプラインと連携させ、一気通貫で自動化する。
* **具体的なワークフロー**:
    1.  ハブPCがデータセット（例: `main_risks_txt.jsonl`）の生成完了を検知する。
    2.  自動的にファインチューニング用のスクリプト（`axolotl`や`transformers.Trainer`を使用）をキックする。
    3.  学習が完了したLoRAアダプタを、モデルリポジトリ（Hugging Face Hubやローカルのストレージ）にバージョン管理して保存する。
    4.  **`Rehab_RAG/evaluation/evaluate_rag.py`**をベースとした評価スクリプトを自動実行し、新しいLoRAアダプタの性能（Faithfulness, Answer Relevancyなど）を測定する。
    5.  性能が基準値を上回っていれば、自動的に本番アプリケーションのモデルとしてデプロイ（差し替え）する。

### 6.2 新規モデル・データソースへの対応
* **新規ベースモデルの追加**: 新しい高性能なSLMが登場した場合、`Modelfile`と設定ファイルを変更するだけで、既存のファインチューニングパイプラインを再利用し、新しいモデル用のLoRAアダプタ群を容易に作成できる設計とする。
* **新規データソースの追加**: J-STAGE以外の論文サイト（CiNii, PubMedなど）や、院内のドキュメントに対応するため、**スクレイピング/テキスト抽出部分をプラグイン化**する。新しいデータソース用のプラグインを作成するだけで、後続の構造化・データセット生成パイプラインはそのまま再利用できるアーキテクチャを目指す。

### 6.3 Human-in-the-Loopの高度化（アクティブラーニング）
* **現状**: 生成されたデータの一部を人間がランダムにレビューする。
* **将来像**: **アクティブラーニング**の概念を導入する。
    * **不確実性サンプリング**: モデルがファインチューニングされる中で、「どのデータで学習すれば最も賢くなるか」を自己判断させる。例えば、モデルが自信を持って判断できなかった生成データや、人間による修正が頻繁に行われたデータを優先的にレビュー対象として提示する。
    * これにより、レビューの労力を最小限に抑えつつ、最も効率的にモデルの性能を向上させることが可能になる。






1. ハブPC（サーバー）用の簡易的な管理Web UIの構築
現状: ハブPCはAPIサーバーとして動作し、ログやフォルダを直接見ることで進捗を確認します。
提案: ハブPCのhub_server.pyに、簡単なWebインターフェース（UI）を追加します。これにより、コマンドラインが苦手な人でも、Webブラウザからシステムの状況を直感的に把握・操作できるようになります。

ダッシュボード機能:

全ジョブ数、未処理、処理中、完了、失敗したジョブの数をリアルタイムで表示。

現在接続しているワーカーPCのリストと、それぞれの最終応答時間（生存確認）。

Gemini APIの総コール数や、推定利用料金の概算を表示。

ジョブ管理機能:

失敗したジョブ（デッドレターキューに入ったもの）の一覧を表示し、内容を確認したり、再実行を指示したりするボタンを設置。

全体の処理を一時停止・再開するボタン。

データレビュー機能:

仕様書に記載した「Human-in-the-Loop」を実現するための画面。生成されたデータセットのサンプルを表示し、「承認」または「却下」を選択できます。

メリット: 複雑な分散システムの状況が可視化され、トラブルシューティングや進捗管理が格段に楽になります。

---

APIキーはPCごとに異なるものを使用する(.envを使用)。
これはレート制限を考えた上での選択です。